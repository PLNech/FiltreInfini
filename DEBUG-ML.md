# ML Model Loading Debug Guide

## Quick Status Check

### 1. Check Background Console
1. Open Firefox: `about:debugging#/runtime/this-firefox`
2. Find FiltreInfini extension
3. Click "Inspect" â†’ Opens background console
4. Look for: `[ML Worker] Script loaded - initializing ML classification worker...`

**Expected**: This message should appear immediately when extension loads
**If missing**: ml-worker.js isn't loading - check manifest.json

### 2. Check UI Console
1. Open manager page (click extension icon)
2. Open browser console (F12)
3. Look for:
   ```
   [ModelPreloader] âœ“ embeddings loaded in 3.1s
   [ModelPreloader] âœ“ classification loaded in 48.3s
   ```

**Expected**: Models load successfully in UI
**Status**: âœ… Working!

### 3. Test Classification
#### Option A: Single Tab (ðŸ§  button)
1. Click ðŸ§  button on any tab in list
2. Check background console for:
   ```
   [ML Worker] Loading model: classification
   [ML Worker] Importing Transformers.js from vendor...
   [ML Worker] Environment configured for local models
   [ML Worker] âœ“ Transformers.js loaded
   [ML Worker] Creating pipeline...
   [ML Worker] âœ“ Model loaded in 18500ms
   ```

#### Option B: ML Debug Modal
1. Click "ðŸ¤– ML Debug" button
2. Click "ðŸ”¬ Classify Single Tab"
3. Should see classification results

#### Option C: Individual Model Tests
1. Click "ðŸ¤– ML Debug" button
2. Try:
   - `ðŸ“Š Test Embeddings` - should work (loads in ~3s)
   - `ðŸŽ¯ Test Classification` - should work (loads in ~20s)
   - `ðŸ”– Test NER` - should work (loads in ~30s)

## Current Status

### âœ… Working
- Model files downloaded (946MB in `lib/vendor/models/`)
- UI model loading (embeddings + classification)
- Popup menu shows model status
- Individual model tests in ML Debug UI

### âš ï¸ Recently Fixed (Test Needed)
- Background worker classification
  - **FIXED**: Export MLClassifierWorker to `self` (Firefox background scope)
  - **FIXED**: Added safety checks before using MLClassifierWorker
  - **TEST**: Check background console for "[ML Worker] Script loaded" message
  - **TEST**: Status should show `{loading: true}` then `{ready: true}`
  - If still failing, check if "MLClassifierWorker not defined" error appears

## Common Issues

### Issue: "Error in input stream"
**Cause**: Trying to load from CDN (not local files)
**Fix**: Check `env.allowRemoteModels = false` and `env.localModelPath` is set

### Issue: "JSON.parse: unexpected end of data"
**Cause**: Model config files are empty (0 bytes)
**Fix**: Re-download with `curl -L` (follow redirects)
```bash
cd lib/vendor/models/embeddings
curl -L "https://huggingface.co/Xenova/all-MiniLM-L6-v2/resolve/main/tokenizer.json" -o tokenizer.json
```

### Issue: "self.registration is undefined"
**Cause**: Using Service Worker API in extension background script
**Fix**: Use `browser.runtime.getURL()` instead of `self.registration.scope`

### Issue: Background worker never loads
**Possible causes**:
1. ml-worker.js has syntax error - check background console
2. Dynamic `import()` failing - check CSP settings
3. Message handlers not receiving messages - add debug logs

## Next Steps

1. **Check background console** for "[ML Worker] Script loaded" message
2. If missing, add more debug logs to ml-worker.js
3. If present, check why `getInstance()` isn't being called
4. **Set up Playwright tests** to automate this checking

## Model Files Structure

```
lib/vendor/models/
â”œâ”€â”€ embeddings/           # 109MB - all-MiniLM-L6-v2
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â””â”€â”€ onnx/
â”‚       â”œâ”€â”€ model.onnx              # 87MB
â”‚       â””â”€â”€ model_quantized.onnx    # 22MB
â”‚
â”œâ”€â”€ classification/       # 321MB - distilbert-base-uncased-mnli
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ vocab.txt
â”‚   â””â”€â”€ onnx/
â”‚       â”œâ”€â”€ model.onnx              # 256MB
â”‚       â””â”€â”€ model_quantized.onnx    # 65MB
â”‚
â””â”€â”€ ner/                 # 516MB - bert-base-NER
    â”œâ”€â”€ config.json
    â”œâ”€â”€ tokenizer.json
    â”œâ”€â”€ tokenizer_config.json
    â”œâ”€â”€ special_tokens_map.json
    â”œâ”€â”€ vocab.txt
    â””â”€â”€ onnx/
        â”œâ”€â”€ model.onnx              # 418MB
        â””â”€â”€ model_quantized.onnx    # 105MB
```

All files should have content (>0 bytes). If any are empty, re-download with curl.
